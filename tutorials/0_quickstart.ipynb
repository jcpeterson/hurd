{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc087614",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "The Human Risky Decision-Making (HURD) Toolkit let's you easily fit risky choice models to data using a high-level API. This includes classic models like Expected Utility Theory (EU) and Prospect Theory (PT) along with contemporary ones, such as the Mixture Of Theories (MOT)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0671e86",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "\n",
    "Before we start fitting models, we'll need to load some human behavioral data.\n",
    "\n",
    "We'll use the large dataset from Peterson et al. (2021), loaded into a `Dataset` object, here called `data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8dd6b097",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<hurd.dataset.Dataset object at 0x7f7508140130>\n"
     ]
    }
   ],
   "source": [
    "# import the dataloader\n",
    "from hurd.internal_datasets import load_c13k_data\n",
    "\n",
    "# import some data\n",
    "data = load_c13k_data()\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c36ce1",
   "metadata": {},
   "source": [
    "Let's take a quick look at the data before we make use of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a8f0ab7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 9831 choice problems in this dataset, each of type <class 'hurd.dataset.Problem'>.\n",
      "\n",
      "For example, the first choice problem is\n",
      "\n",
      "Problem i0-p1:\n",
      "A: Outcomes: [26. -1.], Probabilities: [0.95 0.05], # of outcomes: 2\n",
      "B: Outcomes: [21. 23.], Probabilities: [0.95 0.05], # of outcomes: 2.\n",
      "\n",
      "Participants chose between gamble A and B. They chose B with a rate of 0.63.\n"
     ]
    }
   ],
   "source": [
    "# how many choice problems in the dataset?\n",
    "n_choice_problems = len(data)\n",
    "\n",
    "# look at the first choice problem in the dataset\n",
    "_, choice_problem_1, bRate1 = data.iloc(0)\n",
    "\n",
    "print(\n",
    "    \"There are {} choice problems in this dataset, each of type {}.\".format(\n",
    "        n_choice_problems, type(choice_problem_1)\n",
    "    ),\n",
    "    end=\"\\n\\n\",\n",
    ")\n",
    "\n",
    "print(\n",
    "    \"For example, the first choice problem is\\n\\n{}.\".format(choice_problem_1),\n",
    "    end=\"\\n\\n\",\n",
    ")\n",
    "\n",
    "print(\n",
    "    \"Participants chose between gamble A and B. They chose B with a rate of {:.2f}.\".format(\n",
    "        bRate1\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58904599",
   "metadata": {},
   "source": [
    "## Fitting Models\n",
    "\n",
    "Let's fit a model to the above data. First, we'll need an optimizer to do the parameter fitting.\n",
    "\n",
    "We'll use a gradient-based optimizer called Adam (Kingma & Ba, 2014).\n",
    "\n",
    "We specify at minimum learning rate `lr` and the number of parameter updates `n_iters` to perform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20fca31f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joshuacp/anaconda3/envs/hurd_jax_upgrade_test4/lib/python3.10/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead\n",
      "  warnings.warn('jax.experimental.optimizers is deprecated, '\n"
     ]
    }
   ],
   "source": [
    "from hurd.optimizers import Adam\n",
    "optimizer = Adam(lr=0.01, n_iters=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de38dd0",
   "metadata": {},
   "source": [
    "Now we can import and initialize a model. Let's start with Prospect Theory.\n",
    "\n",
    "We pass in the optimizer and specify a loss function (mean squared error)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0320f07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the model\n",
    "from hurd.models.psychophysical import ProspectTheoryModel\n",
    "\n",
    "# initialize the model\n",
    "pt = ProspectTheoryModel(\n",
    "    optimizer=optimizer,\n",
    "    loss_function=\"mse\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58fd0f45",
   "metadata": {},
   "source": [
    "By default, `ProspectTheoryModel` uses common utility and probability weighting functions from the classic model, but we could also pass them in as arguments, and there's plenty to pick from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aafa1fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# same exact model, just more explicit\n",
    "pt = ProspectTheoryModel(\n",
    "    util_func=\"GeneralPowerLossAverseUtil\",\n",
    "    pwf=\"KT_PWF\",\n",
    "    optimizer=optimizer,\n",
    "    loss_function=\"mse\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c58ed4",
   "metadata": {},
   "source": [
    "Before we fit the model, let's divide the data into non-overlapping train (90%) and validation (10%) sets.\n",
    "\n",
    "The `split` method returns a generator object that we can use to iterate through splits in a loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53a939cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object Dataset.split at 0x7f72306d0510>\n"
     ]
    }
   ],
   "source": [
    "# data can be split with a method yeilding an iterator\n",
    "splitter = data.split(p=0.9, n_splits=1, shuffle=True, random_state=1)\n",
    "print(splitter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0ff530",
   "metadata": {},
   "source": [
    "In this case, we just want one split of the data. The result is two dataset objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4ce92c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<hurd.dataset.Dataset object at 0x7f723cbeff40> <hurd.dataset.Dataset object at 0x7f723cbefe80>\n"
     ]
    }
   ],
   "source": [
    "(train_data, val_data) = list(splitter)[0]\n",
    "print(train_data, val_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9806c99a",
   "metadata": {},
   "source": [
    "Now we are finally ready to fit the Prospect Theory model.\n",
    "\n",
    "Loss for both the training and validation splits is reported for each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d6952dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/200] Train Loss: 0.15614, Val Loss: 0.15019, Elapsed: 2.98s * New Best * \n",
      "[Epoch 2/200] Train Loss: 0.15184, Val Loss: 0.14714, Elapsed: 1.78s * New Best * \n",
      "[Epoch 3/200] Train Loss: 0.14759, Val Loss: 0.14419, Elapsed: 1.67s * New Best * \n",
      "[Epoch 4/200] Train Loss: 0.14335, Val Loss: 0.14086, Elapsed: 0.03s * New Best * \n",
      "[Epoch 5/200] Train Loss: 0.13903, Val Loss: 0.13720, Elapsed: 0.03s * New Best * \n",
      "[Epoch 6/200] Train Loss: 0.13464, Val Loss: 0.13311, Elapsed: 0.03s * New Best * \n",
      "[Epoch 7/200] Train Loss: 0.13015, Val Loss: 0.12868, Elapsed: 0.03s * New Best * \n",
      "[Epoch 8/200] Train Loss: 0.12556, Val Loss: 0.12405, Elapsed: 0.02s * New Best * \n",
      "[Epoch 9/200] Train Loss: 0.12089, Val Loss: 0.11929, Elapsed: 0.03s * New Best * \n",
      "[Epoch 10/200] Train Loss: 0.11615, Val Loss: 0.11438, Elapsed: 0.03s * New Best * \n",
      "[Epoch 11/200] Train Loss: 0.11135, Val Loss: 0.10930, Elapsed: 0.03s * New Best * \n",
      "[Epoch 12/200] Train Loss: 0.10649, Val Loss: 0.10409, Elapsed: 0.03s * New Best * \n",
      "[Epoch 13/200] Train Loss: 0.10158, Val Loss: 0.09880, Elapsed: 0.03s * New Best * \n",
      "[Epoch 14/200] Train Loss: 0.09663, Val Loss: 0.09349, Elapsed: 0.03s * New Best * \n",
      "[Epoch 15/200] Train Loss: 0.09165, Val Loss: 0.08818, Elapsed: 0.03s * New Best * \n",
      "[Epoch 16/200] Train Loss: 0.08664, Val Loss: 0.08291, Elapsed: 0.03s * New Best * \n",
      "[Epoch 17/200] Train Loss: 0.08160, Val Loss: 0.07770, Elapsed: 0.03s * New Best * \n",
      "[Epoch 18/200] Train Loss: 0.07655, Val Loss: 0.07257, Elapsed: 0.02s * New Best * \n",
      "[Epoch 19/200] Train Loss: 0.07150, Val Loss: 0.06754, Elapsed: 0.03s * New Best * \n",
      "[Epoch 20/200] Train Loss: 0.06649, Val Loss: 0.06262, Elapsed: 0.03s * New Best * \n",
      "[Epoch 21/200] Train Loss: 0.06155, Val Loss: 0.05783, Elapsed: 0.03s * New Best * \n",
      "[Epoch 22/200] Train Loss: 0.05675, Val Loss: 0.05321, Elapsed: 0.03s * New Best * \n",
      "[Epoch 23/200] Train Loss: 0.05213, Val Loss: 0.04879, Elapsed: 0.02s * New Best * \n",
      "[Epoch 24/200] Train Loss: 0.04776, Val Loss: 0.04462, Elapsed: 0.03s * New Best * \n",
      "[Epoch 25/200] Train Loss: 0.04370, Val Loss: 0.04076, Elapsed: 0.03s * New Best * \n",
      "[Epoch 26/200] Train Loss: 0.03999, Val Loss: 0.03725, Elapsed: 0.03s * New Best * \n",
      "[Epoch 27/200] Train Loss: 0.03669, Val Loss: 0.03412, Elapsed: 0.03s * New Best * \n",
      "[Epoch 28/200] Train Loss: 0.03381, Val Loss: 0.03140, Elapsed: 0.02s * New Best * \n",
      "[Epoch 29/200] Train Loss: 0.03135, Val Loss: 0.02907, Elapsed: 0.03s * New Best * \n",
      "[Epoch 30/200] Train Loss: 0.02930, Val Loss: 0.02713, Elapsed: 0.03s * New Best * \n",
      "[Epoch 31/200] Train Loss: 0.02762, Val Loss: 0.02555, Elapsed: 0.03s * New Best * \n",
      "[Epoch 32/200] Train Loss: 0.02626, Val Loss: 0.02428, Elapsed: 0.03s * New Best * \n",
      "[Epoch 33/200] Train Loss: 0.02518, Val Loss: 0.02330, Elapsed: 0.03s * New Best * \n",
      "[Epoch 34/200] Train Loss: 0.02435, Val Loss: 0.02256, Elapsed: 0.03s * New Best * \n",
      "[Epoch 35/200] Train Loss: 0.02371, Val Loss: 0.02201, Elapsed: 0.03s * New Best * \n",
      "[Epoch 36/200] Train Loss: 0.02324, Val Loss: 0.02163, Elapsed: 0.02s * New Best * \n",
      "[Epoch 37/200] Train Loss: 0.02291, Val Loss: 0.02138, Elapsed: 0.02s * New Best * \n",
      "[Epoch 38/200] Train Loss: 0.02269, Val Loss: 0.02123, Elapsed: 0.03s * New Best * \n",
      "[Epoch 39/200] Train Loss: 0.02255, Val Loss: 0.02117, Elapsed: 0.03s * New Best * \n",
      "[Epoch 40/200] Train Loss: 0.02248, Val Loss: 0.02116, Elapsed: 0.03s * New Best * \n",
      "[Epoch 41/200] Train Loss: 0.02247, Val Loss: 0.02120, Elapsed: 0.03s\n",
      "[Epoch 42/200] Train Loss: 0.02249, Val Loss: 0.02128, Elapsed: 0.02s\n",
      "[Epoch 43/200] Train Loss: 0.02253, Val Loss: 0.02137, Elapsed: 0.02s\n",
      "[Epoch 44/200] Train Loss: 0.02259, Val Loss: 0.02147, Elapsed: 0.03s\n",
      "[Epoch 45/200] Train Loss: 0.02266, Val Loss: 0.02157, Elapsed: 0.03s\n",
      "[Epoch 46/200] Train Loss: 0.02273, Val Loss: 0.02167, Elapsed: 0.02s\n",
      "[Epoch 47/200] Train Loss: 0.02280, Val Loss: 0.02176, Elapsed: 0.02s\n",
      "[Epoch 48/200] Train Loss: 0.02286, Val Loss: 0.02184, Elapsed: 0.02s\n",
      "[Epoch 49/200] Train Loss: 0.02291, Val Loss: 0.02191, Elapsed: 0.02s\n",
      "[Epoch 50/200] Train Loss: 0.02295, Val Loss: 0.02197, Elapsed: 0.02s\n",
      "[Epoch 51/200] Train Loss: 0.02298, Val Loss: 0.02201, Elapsed: 0.03s\n",
      "[Epoch 52/200] Train Loss: 0.02300, Val Loss: 0.02204, Elapsed: 0.03s\n",
      "[Epoch 53/200] Train Loss: 0.02302, Val Loss: 0.02206, Elapsed: 0.03s\n",
      "[Epoch 54/200] Train Loss: 0.02302, Val Loss: 0.02207, Elapsed: 0.03s\n",
      "[Epoch 55/200] Train Loss: 0.02301, Val Loss: 0.02206, Elapsed: 0.02s\n",
      "[Epoch 56/200] Train Loss: 0.02299, Val Loss: 0.02204, Elapsed: 0.03s\n",
      "[Epoch 57/200] Train Loss: 0.02296, Val Loss: 0.02202, Elapsed: 0.02s\n",
      "[Epoch 58/200] Train Loss: 0.02293, Val Loss: 0.02198, Elapsed: 0.03s\n",
      "[Epoch 59/200] Train Loss: 0.02289, Val Loss: 0.02194, Elapsed: 0.03s\n",
      "[Epoch 60/200] Train Loss: 0.02285, Val Loss: 0.02189, Elapsed: 0.03s\n",
      "[Epoch 61/200] Train Loss: 0.02280, Val Loss: 0.02184, Elapsed: 0.03s\n",
      "[Epoch 62/200] Train Loss: 0.02275, Val Loss: 0.02178, Elapsed: 0.03s\n",
      "[Epoch 63/200] Train Loss: 0.02269, Val Loss: 0.02172, Elapsed: 0.03s\n",
      "[Epoch 64/200] Train Loss: 0.02264, Val Loss: 0.02165, Elapsed: 0.02s\n",
      "[Epoch 65/200] Train Loss: 0.02258, Val Loss: 0.02159, Elapsed: 0.03s\n",
      "[Epoch 66/200] Train Loss: 0.02253, Val Loss: 0.02152, Elapsed: 0.03s\n",
      "[Epoch 67/200] Train Loss: 0.02247, Val Loss: 0.02146, Elapsed: 0.02s\n",
      "[Epoch 68/200] Train Loss: 0.02242, Val Loss: 0.02140, Elapsed: 0.03s\n",
      "[Epoch 69/200] Train Loss: 0.02237, Val Loss: 0.02133, Elapsed: 0.03s\n",
      "[Epoch 70/200] Train Loss: 0.02231, Val Loss: 0.02127, Elapsed: 0.03s\n",
      "[Epoch 71/200] Train Loss: 0.02226, Val Loss: 0.02121, Elapsed: 0.03s\n",
      "[Epoch 72/200] Train Loss: 0.02221, Val Loss: 0.02115, Elapsed: 0.03s * New Best * \n",
      "[Epoch 73/200] Train Loss: 0.02217, Val Loss: 0.02109, Elapsed: 0.02s * New Best * \n",
      "[Epoch 74/200] Train Loss: 0.02212, Val Loss: 0.02104, Elapsed: 0.02s * New Best * \n",
      "[Epoch 75/200] Train Loss: 0.02208, Val Loss: 0.02099, Elapsed: 0.03s * New Best * \n",
      "[Epoch 76/200] Train Loss: 0.02204, Val Loss: 0.02094, Elapsed: 0.03s * New Best * \n",
      "[Epoch 77/200] Train Loss: 0.02200, Val Loss: 0.02090, Elapsed: 0.03s * New Best * \n",
      "[Epoch 78/200] Train Loss: 0.02197, Val Loss: 0.02085, Elapsed: 0.03s * New Best * \n",
      "[Epoch 79/200] Train Loss: 0.02194, Val Loss: 0.02081, Elapsed: 0.02s * New Best * \n",
      "[Epoch 80/200] Train Loss: 0.02191, Val Loss: 0.02078, Elapsed: 0.03s * New Best * \n",
      "[Epoch 81/200] Train Loss: 0.02188, Val Loss: 0.02074, Elapsed: 0.02s * New Best * \n",
      "[Epoch 82/200] Train Loss: 0.02186, Val Loss: 0.02071, Elapsed: 0.03s * New Best * \n",
      "[Epoch 83/200] Train Loss: 0.02183, Val Loss: 0.02068, Elapsed: 0.03s * New Best * \n",
      "[Epoch 84/200] Train Loss: 0.02181, Val Loss: 0.02066, Elapsed: 0.03s * New Best * \n",
      "[Epoch 85/200] Train Loss: 0.02179, Val Loss: 0.02063, Elapsed: 0.03s * New Best * \n",
      "[Epoch 86/200] Train Loss: 0.02177, Val Loss: 0.02061, Elapsed: 0.03s * New Best * \n",
      "[Epoch 87/200] Train Loss: 0.02175, Val Loss: 0.02058, Elapsed: 0.03s * New Best * \n",
      "[Epoch 88/200] Train Loss: 0.02174, Val Loss: 0.02056, Elapsed: 0.03s * New Best * \n",
      "[Epoch 89/200] Train Loss: 0.02172, Val Loss: 0.02054, Elapsed: 0.03s * New Best * \n",
      "[Epoch 90/200] Train Loss: 0.02171, Val Loss: 0.02052, Elapsed: 0.03s * New Best * \n",
      "[Epoch 91/200] Train Loss: 0.02169, Val Loss: 0.02050, Elapsed: 0.03s * New Best * \n",
      "[Epoch 92/200] Train Loss: 0.02168, Val Loss: 0.02049, Elapsed: 0.03s * New Best * \n",
      "[Epoch 93/200] Train Loss: 0.02166, Val Loss: 0.02047, Elapsed: 0.02s * New Best * \n",
      "[Epoch 94/200] Train Loss: 0.02165, Val Loss: 0.02045, Elapsed: 0.03s * New Best * \n",
      "[Epoch 95/200] Train Loss: 0.02163, Val Loss: 0.02044, Elapsed: 0.02s * New Best * \n",
      "[Epoch 96/200] Train Loss: 0.02162, Val Loss: 0.02043, Elapsed: 0.02s * New Best * \n",
      "[Epoch 97/200] Train Loss: 0.02160, Val Loss: 0.02041, Elapsed: 0.03s * New Best * \n",
      "[Epoch 98/200] Train Loss: 0.02159, Val Loss: 0.02040, Elapsed: 0.03s * New Best * \n",
      "[Epoch 99/200] Train Loss: 0.02158, Val Loss: 0.02038, Elapsed: 0.03s * New Best * \n",
      "[Epoch 100/200] Train Loss: 0.02156, Val Loss: 0.02037, Elapsed: 0.03s * New Best * \n",
      "[Epoch 101/200] Train Loss: 0.02155, Val Loss: 0.02036, Elapsed: 0.02s * New Best * \n",
      "[Epoch 102/200] Train Loss: 0.02153, Val Loss: 0.02035, Elapsed: 0.02s * New Best * \n",
      "[Epoch 103/200] Train Loss: 0.02152, Val Loss: 0.02033, Elapsed: 0.02s * New Best * \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 104/200] Train Loss: 0.02151, Val Loss: 0.02032, Elapsed: 0.03s * New Best * \n",
      "[Epoch 105/200] Train Loss: 0.02149, Val Loss: 0.02031, Elapsed: 0.03s * New Best * \n",
      "[Epoch 106/200] Train Loss: 0.02148, Val Loss: 0.02030, Elapsed: 0.03s * New Best * \n",
      "[Epoch 107/200] Train Loss: 0.02147, Val Loss: 0.02029, Elapsed: 0.03s * New Best * \n",
      "[Epoch 108/200] Train Loss: 0.02146, Val Loss: 0.02028, Elapsed: 0.03s * New Best * \n",
      "[Epoch 109/200] Train Loss: 0.02144, Val Loss: 0.02027, Elapsed: 0.03s * New Best * \n",
      "[Epoch 110/200] Train Loss: 0.02143, Val Loss: 0.02026, Elapsed: 0.02s * New Best * \n",
      "[Epoch 111/200] Train Loss: 0.02142, Val Loss: 0.02025, Elapsed: 0.03s * New Best * \n",
      "[Epoch 112/200] Train Loss: 0.02141, Val Loss: 0.02024, Elapsed: 0.03s * New Best * \n",
      "[Epoch 113/200] Train Loss: 0.02140, Val Loss: 0.02023, Elapsed: 0.03s * New Best * \n",
      "[Epoch 114/200] Train Loss: 0.02139, Val Loss: 0.02023, Elapsed: 0.02s * New Best * \n",
      "[Epoch 115/200] Train Loss: 0.02138, Val Loss: 0.02022, Elapsed: 0.03s * New Best * \n",
      "[Epoch 116/200] Train Loss: 0.02136, Val Loss: 0.02021, Elapsed: 0.02s * New Best * \n",
      "[Epoch 117/200] Train Loss: 0.02136, Val Loss: 0.02020, Elapsed: 0.02s * New Best * \n",
      "[Epoch 118/200] Train Loss: 0.02135, Val Loss: 0.02020, Elapsed: 0.03s * New Best * \n",
      "[Epoch 119/200] Train Loss: 0.02134, Val Loss: 0.02019, Elapsed: 0.03s * New Best * \n",
      "[Epoch 120/200] Train Loss: 0.02133, Val Loss: 0.02018, Elapsed: 0.02s * New Best * \n",
      "[Epoch 121/200] Train Loss: 0.02132, Val Loss: 0.02018, Elapsed: 0.03s * New Best * \n",
      "[Epoch 122/200] Train Loss: 0.02131, Val Loss: 0.02017, Elapsed: 0.02s * New Best * \n",
      "[Epoch 123/200] Train Loss: 0.02130, Val Loss: 0.02017, Elapsed: 0.02s * New Best * \n",
      "[Epoch 124/200] Train Loss: 0.02129, Val Loss: 0.02016, Elapsed: 0.02s * New Best * \n",
      "[Epoch 125/200] Train Loss: 0.02129, Val Loss: 0.02015, Elapsed: 0.02s * New Best * \n",
      "[Epoch 126/200] Train Loss: 0.02128, Val Loss: 0.02015, Elapsed: 0.03s * New Best * \n",
      "[Epoch 127/200] Train Loss: 0.02127, Val Loss: 0.02014, Elapsed: 0.03s * New Best * \n",
      "[Epoch 128/200] Train Loss: 0.02126, Val Loss: 0.02014, Elapsed: 0.03s * New Best * \n",
      "[Epoch 129/200] Train Loss: 0.02126, Val Loss: 0.02013, Elapsed: 0.03s * New Best * \n",
      "[Epoch 130/200] Train Loss: 0.02125, Val Loss: 0.02013, Elapsed: 0.03s * New Best * \n",
      "[Epoch 131/200] Train Loss: 0.02124, Val Loss: 0.02012, Elapsed: 0.03s * New Best * \n",
      "[Epoch 132/200] Train Loss: 0.02124, Val Loss: 0.02012, Elapsed: 0.03s * New Best * \n",
      "[Epoch 133/200] Train Loss: 0.02123, Val Loss: 0.02011, Elapsed: 0.03s * New Best * \n",
      "[Epoch 134/200] Train Loss: 0.02123, Val Loss: 0.02011, Elapsed: 0.03s * New Best * \n",
      "[Epoch 135/200] Train Loss: 0.02122, Val Loss: 0.02010, Elapsed: 0.02s * New Best * \n",
      "[Epoch 136/200] Train Loss: 0.02121, Val Loss: 0.02010, Elapsed: 0.02s * New Best * \n",
      "[Epoch 137/200] Train Loss: 0.02121, Val Loss: 0.02010, Elapsed: 0.03s * New Best * \n",
      "[Epoch 138/200] Train Loss: 0.02120, Val Loss: 0.02009, Elapsed: 0.03s * New Best * \n",
      "[Epoch 139/200] Train Loss: 0.02120, Val Loss: 0.02009, Elapsed: 0.03s * New Best * \n",
      "[Epoch 140/200] Train Loss: 0.02119, Val Loss: 0.02008, Elapsed: 0.03s * New Best * \n",
      "[Epoch 141/200] Train Loss: 0.02119, Val Loss: 0.02008, Elapsed: 0.02s * New Best * \n",
      "[Epoch 142/200] Train Loss: 0.02118, Val Loss: 0.02008, Elapsed: 0.02s * New Best * \n",
      "[Epoch 143/200] Train Loss: 0.02118, Val Loss: 0.02007, Elapsed: 0.03s * New Best * \n",
      "[Epoch 144/200] Train Loss: 0.02117, Val Loss: 0.02007, Elapsed: 0.03s * New Best * \n",
      "[Epoch 145/200] Train Loss: 0.02117, Val Loss: 0.02007, Elapsed: 0.03s * New Best * \n",
      "[Epoch 146/200] Train Loss: 0.02117, Val Loss: 0.02007, Elapsed: 0.03s * New Best * \n",
      "[Epoch 147/200] Train Loss: 0.02116, Val Loss: 0.02006, Elapsed: 0.03s * New Best * \n",
      "[Epoch 148/200] Train Loss: 0.02116, Val Loss: 0.02006, Elapsed: 0.03s * New Best * \n",
      "[Epoch 149/200] Train Loss: 0.02115, Val Loss: 0.02006, Elapsed: 0.02s * New Best * \n",
      "[Epoch 150/200] Train Loss: 0.02115, Val Loss: 0.02006, Elapsed: 0.02s * New Best * \n",
      "[Epoch 151/200] Train Loss: 0.02115, Val Loss: 0.02005, Elapsed: 0.03s * New Best * \n",
      "[Epoch 152/200] Train Loss: 0.02114, Val Loss: 0.02005, Elapsed: 0.03s * New Best * \n",
      "[Epoch 153/200] Train Loss: 0.02114, Val Loss: 0.02005, Elapsed: 0.03s * New Best * \n",
      "[Epoch 154/200] Train Loss: 0.02114, Val Loss: 0.02005, Elapsed: 0.03s * New Best * \n",
      "[Epoch 155/200] Train Loss: 0.02113, Val Loss: 0.02005, Elapsed: 0.02s * New Best * \n",
      "[Epoch 156/200] Train Loss: 0.02113, Val Loss: 0.02004, Elapsed: 0.03s * New Best * \n",
      "[Epoch 157/200] Train Loss: 0.02113, Val Loss: 0.02004, Elapsed: 0.03s * New Best * \n",
      "[Epoch 158/200] Train Loss: 0.02113, Val Loss: 0.02004, Elapsed: 0.03s * New Best * \n",
      "[Epoch 159/200] Train Loss: 0.02112, Val Loss: 0.02004, Elapsed: 0.03s * New Best * \n",
      "[Epoch 160/200] Train Loss: 0.02112, Val Loss: 0.02004, Elapsed: 0.03s * New Best * \n",
      "[Epoch 161/200] Train Loss: 0.02112, Val Loss: 0.02004, Elapsed: 0.02s * New Best * \n",
      "[Epoch 162/200] Train Loss: 0.02112, Val Loss: 0.02004, Elapsed: 0.02s * New Best * \n",
      "[Epoch 163/200] Train Loss: 0.02111, Val Loss: 0.02003, Elapsed: 0.03s * New Best * \n",
      "[Epoch 164/200] Train Loss: 0.02111, Val Loss: 0.02003, Elapsed: 0.03s * New Best * \n",
      "[Epoch 165/200] Train Loss: 0.02111, Val Loss: 0.02003, Elapsed: 0.03s * New Best * \n",
      "[Epoch 166/200] Train Loss: 0.02111, Val Loss: 0.02003, Elapsed: 0.03s * New Best * \n",
      "[Epoch 167/200] Train Loss: 0.02110, Val Loss: 0.02003, Elapsed: 0.03s * New Best * \n",
      "[Epoch 168/200] Train Loss: 0.02110, Val Loss: 0.02003, Elapsed: 0.02s * New Best * \n",
      "[Epoch 169/200] Train Loss: 0.02110, Val Loss: 0.02003, Elapsed: 0.02s * New Best * \n",
      "[Epoch 170/200] Train Loss: 0.02110, Val Loss: 0.02003, Elapsed: 0.03s * New Best * \n",
      "[Epoch 171/200] Train Loss: 0.02110, Val Loss: 0.02003, Elapsed: 0.03s * New Best * \n",
      "[Epoch 172/200] Train Loss: 0.02110, Val Loss: 0.02003, Elapsed: 0.02s * New Best * \n",
      "[Epoch 173/200] Train Loss: 0.02109, Val Loss: 0.02003, Elapsed: 0.02s * New Best * \n",
      "[Epoch 174/200] Train Loss: 0.02109, Val Loss: 0.02003, Elapsed: 0.03s * New Best * \n",
      "[Epoch 175/200] Train Loss: 0.02109, Val Loss: 0.02003, Elapsed: 0.03s * New Best * \n",
      "[Epoch 176/200] Train Loss: 0.02109, Val Loss: 0.02003, Elapsed: 0.04s * New Best * \n",
      "[Epoch 177/200] Train Loss: 0.02109, Val Loss: 0.02002, Elapsed: 0.03s * New Best * \n",
      "[Epoch 178/200] Train Loss: 0.02109, Val Loss: 0.02002, Elapsed: 0.03s * New Best * \n",
      "[Epoch 179/200] Train Loss: 0.02109, Val Loss: 0.02002, Elapsed: 0.03s * New Best * \n",
      "[Epoch 180/200] Train Loss: 0.02108, Val Loss: 0.02002, Elapsed: 0.03s * New Best * \n",
      "[Epoch 181/200] Train Loss: 0.02108, Val Loss: 0.02002, Elapsed: 0.03s * New Best * \n",
      "[Epoch 182/200] Train Loss: 0.02108, Val Loss: 0.02002, Elapsed: 0.03s * New Best * \n",
      "[Epoch 183/200] Train Loss: 0.02108, Val Loss: 0.02002, Elapsed: 0.02s * New Best * \n",
      "[Epoch 184/200] Train Loss: 0.02108, Val Loss: 0.02002, Elapsed: 0.02s * New Best * \n",
      "[Epoch 185/200] Train Loss: 0.02108, Val Loss: 0.02002, Elapsed: 0.03s * New Best * \n",
      "[Epoch 186/200] Train Loss: 0.02108, Val Loss: 0.02002, Elapsed: 0.03s * New Best * \n",
      "[Epoch 187/200] Train Loss: 0.02108, Val Loss: 0.02002, Elapsed: 0.02s * New Best * \n",
      "[Epoch 188/200] Train Loss: 0.02108, Val Loss: 0.02002, Elapsed: 0.03s * New Best * \n",
      "[Epoch 189/200] Train Loss: 0.02108, Val Loss: 0.02002, Elapsed: 0.03s * New Best * \n",
      "[Epoch 190/200] Train Loss: 0.02107, Val Loss: 0.02002, Elapsed: 0.02s * New Best * \n",
      "[Epoch 191/200] Train Loss: 0.02107, Val Loss: 0.02002, Elapsed: 0.03s * New Best * \n",
      "[Epoch 192/200] Train Loss: 0.02107, Val Loss: 0.02002, Elapsed: 0.02s * New Best * \n",
      "[Epoch 193/200] Train Loss: 0.02107, Val Loss: 0.02002, Elapsed: 0.03s * New Best * \n",
      "[Epoch 194/200] Train Loss: 0.02107, Val Loss: 0.02002, Elapsed: 0.02s * New Best * \n",
      "[Epoch 195/200] Train Loss: 0.02107, Val Loss: 0.02002, Elapsed: 0.03s * New Best * \n",
      "[Epoch 196/200] Train Loss: 0.02107, Val Loss: 0.02002, Elapsed: 0.03s * New Best * \n",
      "[Epoch 197/200] Train Loss: 0.02107, Val Loss: 0.02002, Elapsed: 0.03s * New Best * \n",
      "[Epoch 198/200] Train Loss: 0.02107, Val Loss: 0.02002, Elapsed: 0.02s * New Best * \n",
      "[Epoch 199/200] Train Loss: 0.02107, Val Loss: 0.02002, Elapsed: 0.02s * New Best * \n",
      "[Epoch 200/200] Train Loss: 0.02107, Val Loss: 0.02002, Elapsed: 0.03s * New Best * \n"
     ]
    }
   ],
   "source": [
    "pt.fit(dataset=train_data, val_dataset=val_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a15604",
   "metadata": {},
   "source": [
    "Now let's train a more complex model, a Mixture Of Theories (MOT) from Peterson et al. (2021).\n",
    "\n",
    "Note that it performs much better than Prospect Theory on this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "99b67bff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/200] Train Loss: 0.11413, Val Loss: 0.11290, Elapsed: 5.20s\n",
      "[Epoch 2/200] Train Loss: 0.10674, Val Loss: 0.10638, Elapsed: 3.60s\n",
      "[Epoch 3/200] Train Loss: 0.10145, Val Loss: 0.10229, Elapsed: 3.71s\n",
      "[Epoch 4/200] Train Loss: 0.09710, Val Loss: 0.09874, Elapsed: 0.06s\n",
      "[Epoch 5/200] Train Loss: 0.09286, Val Loss: 0.09500, Elapsed: 0.05s\n",
      "[Epoch 6/200] Train Loss: 0.08855, Val Loss: 0.09109, Elapsed: 0.06s\n",
      "[Epoch 7/200] Train Loss: 0.08427, Val Loss: 0.08707, Elapsed: 0.05s\n",
      "[Epoch 8/200] Train Loss: 0.08006, Val Loss: 0.08297, Elapsed: 0.05s\n",
      "[Epoch 9/200] Train Loss: 0.07592, Val Loss: 0.07883, Elapsed: 0.05s\n",
      "[Epoch 10/200] Train Loss: 0.07188, Val Loss: 0.07466, Elapsed: 0.05s\n",
      "[Epoch 11/200] Train Loss: 0.06794, Val Loss: 0.07050, Elapsed: 0.05s\n",
      "[Epoch 12/200] Train Loss: 0.06410, Val Loss: 0.06636, Elapsed: 0.05s\n",
      "[Epoch 13/200] Train Loss: 0.06038, Val Loss: 0.06228, Elapsed: 0.05s\n",
      "[Epoch 14/200] Train Loss: 0.05678, Val Loss: 0.05831, Elapsed: 0.05s\n",
      "[Epoch 15/200] Train Loss: 0.05330, Val Loss: 0.05446, Elapsed: 0.05s\n",
      "[Epoch 16/200] Train Loss: 0.04997, Val Loss: 0.05076, Elapsed: 0.05s\n",
      "[Epoch 17/200] Train Loss: 0.04679, Val Loss: 0.04723, Elapsed: 0.05s\n",
      "[Epoch 18/200] Train Loss: 0.04377, Val Loss: 0.04388, Elapsed: 0.05s\n",
      "[Epoch 19/200] Train Loss: 0.04093, Val Loss: 0.04069, Elapsed: 0.06s\n",
      "[Epoch 20/200] Train Loss: 0.03827, Val Loss: 0.03770, Elapsed: 0.05s\n",
      "[Epoch 21/200] Train Loss: 0.03578, Val Loss: 0.03490, Elapsed: 0.05s\n",
      "[Epoch 22/200] Train Loss: 0.03349, Val Loss: 0.03231, Elapsed: 0.06s\n",
      "[Epoch 23/200] Train Loss: 0.03138, Val Loss: 0.02995, Elapsed: 0.06s\n",
      "[Epoch 24/200] Train Loss: 0.02945, Val Loss: 0.02778, Elapsed: 0.05s\n",
      "[Epoch 25/200] Train Loss: 0.02770, Val Loss: 0.02585, Elapsed: 0.05s\n",
      "[Epoch 26/200] Train Loss: 0.02613, Val Loss: 0.02412, Elapsed: 0.05s\n",
      "[Epoch 27/200] Train Loss: 0.02470, Val Loss: 0.02262, Elapsed: 0.05s\n",
      "[Epoch 28/200] Train Loss: 0.02341, Val Loss: 0.02129, Elapsed: 0.05s\n",
      "[Epoch 29/200] Train Loss: 0.02224, Val Loss: 0.02007, Elapsed: 0.05s\n",
      "[Epoch 30/200] Train Loss: 0.02119, Val Loss: 0.01903, Elapsed: 0.05s * New Best * \n",
      "[Epoch 31/200] Train Loss: 0.02024, Val Loss: 0.01810, Elapsed: 0.05s * New Best * \n",
      "[Epoch 32/200] Train Loss: 0.01938, Val Loss: 0.01729, Elapsed: 0.06s * New Best * \n",
      "[Epoch 33/200] Train Loss: 0.01863, Val Loss: 0.01657, Elapsed: 0.05s * New Best * \n",
      "[Epoch 34/200] Train Loss: 0.01795, Val Loss: 0.01594, Elapsed: 0.05s * New Best * \n",
      "[Epoch 35/200] Train Loss: 0.01736, Val Loss: 0.01538, Elapsed: 0.05s * New Best * \n",
      "[Epoch 36/200] Train Loss: 0.01685, Val Loss: 0.01490, Elapsed: 0.05s * New Best * \n",
      "[Epoch 37/200] Train Loss: 0.01642, Val Loss: 0.01448, Elapsed: 0.05s * New Best * \n",
      "[Epoch 38/200] Train Loss: 0.01604, Val Loss: 0.01415, Elapsed: 0.05s * New Best * \n",
      "[Epoch 39/200] Train Loss: 0.01572, Val Loss: 0.01387, Elapsed: 0.06s * New Best * \n",
      "[Epoch 40/200] Train Loss: 0.01545, Val Loss: 0.01363, Elapsed: 0.05s * New Best * \n",
      "[Epoch 41/200] Train Loss: 0.01522, Val Loss: 0.01344, Elapsed: 0.05s * New Best * \n",
      "[Epoch 42/200] Train Loss: 0.01502, Val Loss: 0.01328, Elapsed: 0.05s * New Best * \n",
      "[Epoch 43/200] Train Loss: 0.01486, Val Loss: 0.01314, Elapsed: 0.05s * New Best * \n",
      "[Epoch 44/200] Train Loss: 0.01473, Val Loss: 0.01304, Elapsed: 0.05s * New Best * \n",
      "[Epoch 45/200] Train Loss: 0.01461, Val Loss: 0.01295, Elapsed: 0.05s * New Best * \n",
      "[Epoch 46/200] Train Loss: 0.01452, Val Loss: 0.01289, Elapsed: 0.06s * New Best * \n",
      "[Epoch 47/200] Train Loss: 0.01445, Val Loss: 0.01284, Elapsed: 0.05s * New Best * \n",
      "[Epoch 48/200] Train Loss: 0.01438, Val Loss: 0.01280, Elapsed: 0.05s * New Best * \n",
      "[Epoch 49/200] Train Loss: 0.01434, Val Loss: 0.01277, Elapsed: 0.05s * New Best * \n",
      "[Epoch 50/200] Train Loss: 0.01430, Val Loss: 0.01275, Elapsed: 0.07s * New Best * \n",
      "[Epoch 51/200] Train Loss: 0.01428, Val Loss: 0.01274, Elapsed: 0.06s * New Best * \n",
      "[Epoch 52/200] Train Loss: 0.01426, Val Loss: 0.01272, Elapsed: 0.05s * New Best * \n",
      "[Epoch 53/200] Train Loss: 0.01424, Val Loss: 0.01271, Elapsed: 0.06s * New Best * \n",
      "[Epoch 54/200] Train Loss: 0.01423, Val Loss: 0.01270, Elapsed: 0.05s * New Best * \n",
      "[Epoch 55/200] Train Loss: 0.01422, Val Loss: 0.01269, Elapsed: 0.06s * New Best * \n",
      "[Epoch 56/200] Train Loss: 0.01421, Val Loss: 0.01269, Elapsed: 0.05s * New Best * \n",
      "[Epoch 57/200] Train Loss: 0.01420, Val Loss: 0.01269, Elapsed: 0.06s * New Best * \n",
      "[Epoch 58/200] Train Loss: 0.01419, Val Loss: 0.01269, Elapsed: 0.06s * New Best * \n",
      "[Epoch 59/200] Train Loss: 0.01419, Val Loss: 0.01269, Elapsed: 0.05s * New Best * \n",
      "[Epoch 60/200] Train Loss: 0.01418, Val Loss: 0.01268, Elapsed: 0.05s * New Best * \n",
      "[Epoch 61/200] Train Loss: 0.01417, Val Loss: 0.01268, Elapsed: 0.05s * New Best * \n",
      "[Epoch 62/200] Train Loss: 0.01415, Val Loss: 0.01268, Elapsed: 0.05s * New Best * \n",
      "[Epoch 63/200] Train Loss: 0.01414, Val Loss: 0.01268, Elapsed: 0.05s * New Best * \n",
      "[Epoch 64/200] Train Loss: 0.01413, Val Loss: 0.01267, Elapsed: 0.06s * New Best * \n",
      "[Epoch 65/200] Train Loss: 0.01411, Val Loss: 0.01267, Elapsed: 0.05s * New Best * \n",
      "[Epoch 66/200] Train Loss: 0.01410, Val Loss: 0.01266, Elapsed: 0.05s * New Best * \n",
      "[Epoch 67/200] Train Loss: 0.01408, Val Loss: 0.01266, Elapsed: 0.05s * New Best * \n",
      "[Epoch 68/200] Train Loss: 0.01407, Val Loss: 0.01264, Elapsed: 0.05s * New Best * \n",
      "[Epoch 69/200] Train Loss: 0.01405, Val Loss: 0.01263, Elapsed: 0.05s * New Best * \n",
      "[Epoch 70/200] Train Loss: 0.01402, Val Loss: 0.01261, Elapsed: 0.05s * New Best * \n",
      "[Epoch 71/200] Train Loss: 0.01400, Val Loss: 0.01259, Elapsed: 0.05s * New Best * \n",
      "[Epoch 72/200] Train Loss: 0.01398, Val Loss: 0.01257, Elapsed: 0.06s * New Best * \n",
      "[Epoch 73/200] Train Loss: 0.01395, Val Loss: 0.01254, Elapsed: 0.05s * New Best * \n",
      "[Epoch 74/200] Train Loss: 0.01393, Val Loss: 0.01251, Elapsed: 0.05s * New Best * \n",
      "[Epoch 75/200] Train Loss: 0.01391, Val Loss: 0.01249, Elapsed: 0.05s * New Best * \n",
      "[Epoch 76/200] Train Loss: 0.01388, Val Loss: 0.01247, Elapsed: 0.05s * New Best * \n",
      "[Epoch 77/200] Train Loss: 0.01386, Val Loss: 0.01245, Elapsed: 0.05s * New Best * \n",
      "[Epoch 78/200] Train Loss: 0.01384, Val Loss: 0.01243, Elapsed: 0.06s * New Best * \n",
      "[Epoch 79/200] Train Loss: 0.01381, Val Loss: 0.01241, Elapsed: 0.05s * New Best * \n",
      "[Epoch 80/200] Train Loss: 0.01379, Val Loss: 0.01238, Elapsed: 0.05s * New Best * \n",
      "[Epoch 81/200] Train Loss: 0.01377, Val Loss: 0.01236, Elapsed: 0.05s * New Best * \n",
      "[Epoch 82/200] Train Loss: 0.01374, Val Loss: 0.01234, Elapsed: 0.05s * New Best * \n",
      "[Epoch 83/200] Train Loss: 0.01372, Val Loss: 0.01232, Elapsed: 0.06s * New Best * \n",
      "[Epoch 84/200] Train Loss: 0.01370, Val Loss: 0.01230, Elapsed: 0.05s * New Best * \n",
      "[Epoch 85/200] Train Loss: 0.01368, Val Loss: 0.01229, Elapsed: 0.05s * New Best * \n",
      "[Epoch 86/200] Train Loss: 0.01366, Val Loss: 0.01227, Elapsed: 0.05s * New Best * \n",
      "[Epoch 87/200] Train Loss: 0.01364, Val Loss: 0.01226, Elapsed: 0.05s * New Best * \n",
      "[Epoch 88/200] Train Loss: 0.01363, Val Loss: 0.01224, Elapsed: 0.05s * New Best * \n",
      "[Epoch 89/200] Train Loss: 0.01361, Val Loss: 0.01223, Elapsed: 0.06s * New Best * \n",
      "[Epoch 90/200] Train Loss: 0.01360, Val Loss: 0.01222, Elapsed: 0.06s * New Best * \n",
      "[Epoch 91/200] Train Loss: 0.01359, Val Loss: 0.01221, Elapsed: 0.06s * New Best * \n",
      "[Epoch 92/200] Train Loss: 0.01357, Val Loss: 0.01221, Elapsed: 0.05s * New Best * \n",
      "[Epoch 93/200] Train Loss: 0.01356, Val Loss: 0.01220, Elapsed: 0.05s * New Best * \n",
      "[Epoch 94/200] Train Loss: 0.01355, Val Loss: 0.01220, Elapsed: 0.05s * New Best * \n",
      "[Epoch 95/200] Train Loss: 0.01354, Val Loss: 0.01219, Elapsed: 0.05s * New Best * \n",
      "[Epoch 96/200] Train Loss: 0.01353, Val Loss: 0.01219, Elapsed: 0.05s * New Best * \n",
      "[Epoch 97/200] Train Loss: 0.01352, Val Loss: 0.01218, Elapsed: 0.05s * New Best * \n",
      "[Epoch 98/200] Train Loss: 0.01351, Val Loss: 0.01218, Elapsed: 0.05s * New Best * \n",
      "[Epoch 99/200] Train Loss: 0.01350, Val Loss: 0.01218, Elapsed: 0.05s * New Best * \n",
      "[Epoch 100/200] Train Loss: 0.01349, Val Loss: 0.01218, Elapsed: 0.05s * New Best * \n",
      "[Epoch 101/200] Train Loss: 0.01348, Val Loss: 0.01218, Elapsed: 0.05s * New Best * \n",
      "[Epoch 102/200] Train Loss: 0.01347, Val Loss: 0.01217, Elapsed: 0.05s * New Best * \n",
      "[Epoch 103/200] Train Loss: 0.01346, Val Loss: 0.01217, Elapsed: 0.05s * New Best * \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 104/200] Train Loss: 0.01346, Val Loss: 0.01217, Elapsed: 0.05s * New Best * \n",
      "[Epoch 105/200] Train Loss: 0.01345, Val Loss: 0.01217, Elapsed: 0.06s * New Best * \n",
      "[Epoch 106/200] Train Loss: 0.01344, Val Loss: 0.01217, Elapsed: 0.05s * New Best * \n",
      "[Epoch 107/200] Train Loss: 0.01343, Val Loss: 0.01217, Elapsed: 0.05s * New Best * \n",
      "[Epoch 108/200] Train Loss: 0.01343, Val Loss: 0.01217, Elapsed: 0.06s * New Best * \n",
      "[Epoch 109/200] Train Loss: 0.01342, Val Loss: 0.01216, Elapsed: 0.05s * New Best * \n",
      "[Epoch 110/200] Train Loss: 0.01341, Val Loss: 0.01216, Elapsed: 0.05s * New Best * \n",
      "[Epoch 111/200] Train Loss: 0.01341, Val Loss: 0.01216, Elapsed: 0.05s * New Best * \n",
      "[Epoch 112/200] Train Loss: 0.01340, Val Loss: 0.01215, Elapsed: 0.06s * New Best * \n",
      "[Epoch 113/200] Train Loss: 0.01339, Val Loss: 0.01215, Elapsed: 0.05s * New Best * \n",
      "[Epoch 114/200] Train Loss: 0.01338, Val Loss: 0.01215, Elapsed: 0.05s * New Best * \n",
      "[Epoch 115/200] Train Loss: 0.01338, Val Loss: 0.01214, Elapsed: 0.05s * New Best * \n",
      "[Epoch 116/200] Train Loss: 0.01337, Val Loss: 0.01214, Elapsed: 0.05s * New Best * \n",
      "[Epoch 117/200] Train Loss: 0.01336, Val Loss: 0.01213, Elapsed: 0.05s * New Best * \n",
      "[Epoch 118/200] Train Loss: 0.01336, Val Loss: 0.01213, Elapsed: 0.05s * New Best * \n",
      "[Epoch 119/200] Train Loss: 0.01335, Val Loss: 0.01213, Elapsed: 0.05s * New Best * \n",
      "[Epoch 120/200] Train Loss: 0.01335, Val Loss: 0.01213, Elapsed: 0.05s * New Best * \n",
      "[Epoch 121/200] Train Loss: 0.01334, Val Loss: 0.01212, Elapsed: 0.05s * New Best * \n",
      "[Epoch 122/200] Train Loss: 0.01333, Val Loss: 0.01212, Elapsed: 0.06s * New Best * \n",
      "[Epoch 123/200] Train Loss: 0.01333, Val Loss: 0.01212, Elapsed: 0.06s * New Best * \n",
      "[Epoch 124/200] Train Loss: 0.01332, Val Loss: 0.01212, Elapsed: 0.05s * New Best * \n",
      "[Epoch 125/200] Train Loss: 0.01332, Val Loss: 0.01212, Elapsed: 0.05s * New Best * \n",
      "[Epoch 126/200] Train Loss: 0.01331, Val Loss: 0.01212, Elapsed: 0.05s * New Best * \n",
      "[Epoch 127/200] Train Loss: 0.01331, Val Loss: 0.01211, Elapsed: 0.05s * New Best * \n",
      "[Epoch 128/200] Train Loss: 0.01330, Val Loss: 0.01211, Elapsed: 0.05s * New Best * \n",
      "[Epoch 129/200] Train Loss: 0.01329, Val Loss: 0.01211, Elapsed: 0.05s * New Best * \n",
      "[Epoch 130/200] Train Loss: 0.01329, Val Loss: 0.01211, Elapsed: 0.05s * New Best * \n",
      "[Epoch 131/200] Train Loss: 0.01328, Val Loss: 0.01210, Elapsed: 0.06s * New Best * \n",
      "[Epoch 132/200] Train Loss: 0.01328, Val Loss: 0.01210, Elapsed: 0.05s * New Best * \n",
      "[Epoch 133/200] Train Loss: 0.01327, Val Loss: 0.01210, Elapsed: 0.07s * New Best * \n",
      "[Epoch 134/200] Train Loss: 0.01327, Val Loss: 0.01209, Elapsed: 0.06s * New Best * \n",
      "[Epoch 135/200] Train Loss: 0.01326, Val Loss: 0.01209, Elapsed: 0.06s * New Best * \n",
      "[Epoch 136/200] Train Loss: 0.01326, Val Loss: 0.01209, Elapsed: 0.05s * New Best * \n",
      "[Epoch 137/200] Train Loss: 0.01325, Val Loss: 0.01208, Elapsed: 0.06s * New Best * \n",
      "[Epoch 138/200] Train Loss: 0.01324, Val Loss: 0.01208, Elapsed: 0.05s * New Best * \n",
      "[Epoch 139/200] Train Loss: 0.01324, Val Loss: 0.01207, Elapsed: 0.06s * New Best * \n",
      "[Epoch 140/200] Train Loss: 0.01323, Val Loss: 0.01207, Elapsed: 0.05s * New Best * \n",
      "[Epoch 141/200] Train Loss: 0.01323, Val Loss: 0.01207, Elapsed: 0.06s * New Best * \n",
      "[Epoch 142/200] Train Loss: 0.01322, Val Loss: 0.01207, Elapsed: 0.05s * New Best * \n",
      "[Epoch 143/200] Train Loss: 0.01322, Val Loss: 0.01207, Elapsed: 0.05s * New Best * \n",
      "[Epoch 144/200] Train Loss: 0.01321, Val Loss: 0.01207, Elapsed: 0.06s * New Best * \n",
      "[Epoch 145/200] Train Loss: 0.01321, Val Loss: 0.01206, Elapsed: 0.05s * New Best * \n",
      "[Epoch 146/200] Train Loss: 0.01320, Val Loss: 0.01206, Elapsed: 0.06s * New Best * \n",
      "[Epoch 147/200] Train Loss: 0.01320, Val Loss: 0.01206, Elapsed: 0.06s * New Best * \n",
      "[Epoch 148/200] Train Loss: 0.01319, Val Loss: 0.01206, Elapsed: 0.05s * New Best * \n",
      "[Epoch 149/200] Train Loss: 0.01319, Val Loss: 0.01206, Elapsed: 0.06s * New Best * \n",
      "[Epoch 150/200] Train Loss: 0.01318, Val Loss: 0.01206, Elapsed: 0.05s * New Best * \n",
      "[Epoch 151/200] Train Loss: 0.01318, Val Loss: 0.01206, Elapsed: 0.05s * New Best * \n",
      "[Epoch 152/200] Train Loss: 0.01317, Val Loss: 0.01206, Elapsed: 0.06s * New Best * \n",
      "[Epoch 153/200] Train Loss: 0.01317, Val Loss: 0.01206, Elapsed: 0.05s * New Best * \n",
      "[Epoch 154/200] Train Loss: 0.01316, Val Loss: 0.01206, Elapsed: 0.06s * New Best * \n",
      "[Epoch 155/200] Train Loss: 0.01316, Val Loss: 0.01206, Elapsed: 0.05s * New Best * \n",
      "[Epoch 156/200] Train Loss: 0.01315, Val Loss: 0.01206, Elapsed: 0.05s * New Best * \n",
      "[Epoch 157/200] Train Loss: 0.01315, Val Loss: 0.01206, Elapsed: 0.05s * New Best * \n",
      "[Epoch 158/200] Train Loss: 0.01314, Val Loss: 0.01206, Elapsed: 0.05s * New Best * \n",
      "[Epoch 159/200] Train Loss: 0.01314, Val Loss: 0.01205, Elapsed: 0.05s * New Best * \n",
      "[Epoch 160/200] Train Loss: 0.01313, Val Loss: 0.01205, Elapsed: 0.05s * New Best * \n",
      "[Epoch 161/200] Train Loss: 0.01313, Val Loss: 0.01205, Elapsed: 0.05s * New Best * \n",
      "[Epoch 162/200] Train Loss: 0.01313, Val Loss: 0.01205, Elapsed: 0.06s * New Best * \n",
      "[Epoch 163/200] Train Loss: 0.01312, Val Loss: 0.01205, Elapsed: 0.05s * New Best * \n",
      "[Epoch 164/200] Train Loss: 0.01312, Val Loss: 0.01205, Elapsed: 0.05s * New Best * \n",
      "[Epoch 165/200] Train Loss: 0.01311, Val Loss: 0.01205, Elapsed: 0.05s * New Best * \n",
      "[Epoch 166/200] Train Loss: 0.01311, Val Loss: 0.01205, Elapsed: 0.05s * New Best * \n",
      "[Epoch 167/200] Train Loss: 0.01310, Val Loss: 0.01205, Elapsed: 0.05s * New Best * \n",
      "[Epoch 168/200] Train Loss: 0.01310, Val Loss: 0.01205, Elapsed: 0.05s * New Best * \n",
      "[Epoch 169/200] Train Loss: 0.01309, Val Loss: 0.01205, Elapsed: 0.05s\n",
      "[Epoch 170/200] Train Loss: 0.01309, Val Loss: 0.01205, Elapsed: 0.05s\n",
      "[Epoch 171/200] Train Loss: 0.01309, Val Loss: 0.01205, Elapsed: 0.05s\n",
      "[Epoch 172/200] Train Loss: 0.01308, Val Loss: 0.01205, Elapsed: 0.05s\n",
      "[Epoch 173/200] Train Loss: 0.01308, Val Loss: 0.01205, Elapsed: 0.05s\n",
      "[Epoch 174/200] Train Loss: 0.01307, Val Loss: 0.01205, Elapsed: 0.05s\n",
      "[Epoch 175/200] Train Loss: 0.01307, Val Loss: 0.01205, Elapsed: 0.05s\n",
      "[Epoch 176/200] Train Loss: 0.01306, Val Loss: 0.01205, Elapsed: 0.06s\n",
      "[Epoch 177/200] Train Loss: 0.01306, Val Loss: 0.01205, Elapsed: 0.06s\n",
      "[Epoch 178/200] Train Loss: 0.01305, Val Loss: 0.01205, Elapsed: 0.06s\n",
      "[Epoch 179/200] Train Loss: 0.01305, Val Loss: 0.01205, Elapsed: 0.05s\n",
      "[Epoch 180/200] Train Loss: 0.01304, Val Loss: 0.01205, Elapsed: 0.05s\n",
      "[Epoch 181/200] Train Loss: 0.01304, Val Loss: 0.01205, Elapsed: 0.05s\n",
      "[Epoch 182/200] Train Loss: 0.01303, Val Loss: 0.01205, Elapsed: 0.05s\n",
      "[Epoch 183/200] Train Loss: 0.01303, Val Loss: 0.01205, Elapsed: 0.05s\n",
      "[Epoch 184/200] Train Loss: 0.01302, Val Loss: 0.01205, Elapsed: 0.05s\n",
      "[Epoch 185/200] Train Loss: 0.01302, Val Loss: 0.01205, Elapsed: 0.05s\n",
      "[Epoch 186/200] Train Loss: 0.01301, Val Loss: 0.01205, Elapsed: 0.05s\n",
      "[Epoch 187/200] Train Loss: 0.01301, Val Loss: 0.01205, Elapsed: 0.05s\n",
      "[Epoch 188/200] Train Loss: 0.01300, Val Loss: 0.01205, Elapsed: 0.05s\n",
      "[Epoch 189/200] Train Loss: 0.01300, Val Loss: 0.01205, Elapsed: 0.05s\n",
      "[Epoch 190/200] Train Loss: 0.01299, Val Loss: 0.01205, Elapsed: 0.05s\n",
      "[Epoch 191/200] Train Loss: 0.01299, Val Loss: 0.01204, Elapsed: 0.05s * New Best * \n",
      "[Epoch 192/200] Train Loss: 0.01298, Val Loss: 0.01204, Elapsed: 0.06s * New Best * \n",
      "[Epoch 193/200] Train Loss: 0.01298, Val Loss: 0.01204, Elapsed: 0.06s * New Best * \n",
      "[Epoch 194/200] Train Loss: 0.01297, Val Loss: 0.01204, Elapsed: 0.05s * New Best * \n",
      "[Epoch 195/200] Train Loss: 0.01297, Val Loss: 0.01204, Elapsed: 0.06s * New Best * \n",
      "[Epoch 196/200] Train Loss: 0.01296, Val Loss: 0.01203, Elapsed: 0.05s * New Best * \n",
      "[Epoch 197/200] Train Loss: 0.01295, Val Loss: 0.01203, Elapsed: 0.05s * New Best * \n",
      "[Epoch 198/200] Train Loss: 0.01295, Val Loss: 0.01203, Elapsed: 0.04s * New Best * \n",
      "[Epoch 199/200] Train Loss: 0.01294, Val Loss: 0.01203, Elapsed: 0.05s * New Best * \n",
      "[Epoch 200/200] Train Loss: 0.01294, Val Loss: 0.01203, Elapsed: 0.06s * New Best * \n"
     ]
    }
   ],
   "source": [
    "from hurd.models.mixture_of_theories import MixtureOfTheories\n",
    "\n",
    "mot = MixtureOfTheories(\n",
    "    optimizer=optimizer,\n",
    "    loss_function=\"mse\",\n",
    ")\n",
    "\n",
    "mot.fit(dataset=train_data, val_dataset=val_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:hurd_jax_upgrade_test4]",
   "language": "python",
   "name": "conda-env-hurd_jax_upgrade_test4-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
